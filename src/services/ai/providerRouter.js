/**
 * Router centralizado para selecci칩n de proveedor de IA
 * Punto 칰nico de decisi칩n entre todos los proveedores soportados
 */

import { getSettings } from '../settingsService';
import { sendToGemini, sendToGeminiStreaming } from './geminiProvider';
import { sendToDeepseek, sendToDeepseekStreaming, getDeepseekAvailableModels } from './deepseekProvider';
import { sendToKimi, sendToKimiStreaming, getKimiAvailableModels } from './kimiProvider';
import { sendToLMStudio, sendToLMStudioStreaming, getLMStudioModels } from './lmStudioProvider';
import { generateContent as ollamaGenerate, generateContentStreaming as ollamaGenerateStreaming } from './ollamaProvider';

/**
 * Env칤a un prompt al proveedor de IA configurado en settings
 * @param {string} prompt - Prompt completo (ya construido con contexto si aplica)
 * @param {Object} options - Opciones adicionales (ej: { format: 'json' } para Ollama)
 * @returns {Promise<{text: string, provider: string}>}
 */
export async function callProvider(prompt, options = {}) {
  const settings = await getSettings();
  const provider = settings.aiProvider || 'ollama';

  switch (provider) {
    case 'ollama': {
      // Permitir override del modelo v칤a options.model > options.ragModel > settings
      const model = options.model || options.ragModel || settings.ollamaModel;
      if (!model) {
        throw new Error('No se ha seleccionado un modelo de Ollama en los ajustes.');
      }
      const response = await ollamaGenerate(model, prompt, options);
      return {
        text: response || 'Sin respuesta',
        provider: 'ollama',
        model: model // Devolvemos el modelo usado para debug
      };
    }

    case 'lmstudio': {
      const response = await sendToLMStudio(prompt);
      return {
        text: response || 'Sin respuesta',
        provider: 'lmstudio'
      };
    }

    case 'deepseek': {
      if (!settings.deepseekApiKey) {
        throw new Error('No se ha configurado la DeepSeek API Key en los ajustes.');
      }
      const response = await sendToDeepseek(prompt, options.model || null);
      return {
        text: response || 'Sin respuesta',
        provider: 'deepseek'
      };
    }

    case 'kimi': {
      if (!settings.kimiApiKey) {
        throw new Error('No se ha configurado la Kimi API Key en los ajustes.');
      }
      const response = await sendToKimi(prompt, options.model || null);
      return {
        text: response || 'Sin respuesta',
        provider: 'kimi'
      };
    }

    case 'gemini': {
      if (!settings.geminiApiKey) {
        throw new Error('No se ha configurado la Gemini API Key en los ajustes.');
      }
      const result = await sendToGemini(prompt, true, false); // useFreeTier = false
      const text = result?.candidates?.[0]?.content?.parts?.[0]?.text || 'Sin respuesta';
      return {
        text,
        provider: 'gemini'
      };
    }

    case 'geminifree':
    default: {
      // Gemini Free usa la misma implementaci칩n que Gemini Pro
      // Pero puede tener una API key diferente
      const apiKey = settings.geminiFreeApiKey || settings.geminiApiKey;
      if (!apiKey) {
        throw new Error('No se ha configurado la Gemini Free API Key en los ajustes.');
      }
      // Usar sendToGemini con useFreeTier = true
      const result = await sendToGemini(prompt, true, true);
      const text = result?.candidates?.[0]?.content?.parts?.[0]?.text || 'Sin respuesta';
      return {
        text,
        provider: 'geminifree'
      };
    }
  }
}

/**
 * Valida la configuraci칩n del proveedor de IA actual
 * @returns {Promise<{valid: boolean, error: string|null}>}
 */
export async function validateProviderConfig() {
  try {
    const settings = await getSettings();
    const provider = settings.aiProvider || 'geminifree';

    switch (provider) {
      case 'geminifree':
        if (!settings.geminiFreeApiKey && !settings.geminiApiKey) {
          return { valid: false, error: 'Falta configurar la Gemini Free API Key' };
        }
        break;
      case 'gemini':
        if (!settings.geminiApiKey) {
          return { valid: false, error: 'Falta configurar la Gemini API Key' };
        }
        break;
      case 'deepseek':
        if (!settings.deepseekApiKey) {
          return { valid: false, error: 'Falta configurar la DeepSeek API Key' };
        }
        break;
      case 'kimi':
        if (!settings.kimiApiKey) {
          return { valid: false, error: 'Falta configurar la Kimi API Key' };
        }
        break;
      case 'ollama':
        if (!settings.ollamaModel) {
          return { valid: false, error: 'Falta seleccionar un modelo de Ollama' };
        }
        break;
      case 'lmstudio':
        if (!settings.lmStudioModel) {
          return { valid: false, error: 'Falta seleccionar un modelo en LM Studio' };
        }
        break;
    }

    return { valid: true, error: null };
  } catch (error) {
    return { valid: false, error: error.message };
  }
}

/**
 * Env칤a un prompt al proveedor de IA configurado con soporte para streaming
 * Todos los proveedores soportan streaming nativamente
 * @param {string} prompt - Prompt completo (ya construido con contexto si aplica)
 * @param {Function} onChunk - Callback que recibe cada chunk de la respuesta
 * @param {Object} options - Opciones adicionales (ej: { ragModel: 'deepseek-r1' })
 * @returns {Promise<{text: string, provider: string, streaming: boolean}>}
 */
export async function callProviderStreaming(prompt, onChunk, options = {}) {
  const settings = await getSettings();
  const provider = settings.aiProvider || 'geminifree';

  console.log(`[callProviderStreaming] Provider: ${provider}`);

  switch (provider) {
    case 'geminifree': {
      console.log('[callProviderStreaming] Iniciando streaming con Gemini Free');
      const fullResponse = await sendToGeminiStreaming(prompt, onChunk, true); // useFreeTier = true
      return {
        text: fullResponse || 'Sin respuesta',
        provider: 'geminifree',
        streaming: true
      };
    }

    case 'gemini': {
      console.log('[callProviderStreaming] Iniciando streaming con Gemini Pro');
      const fullResponse = await sendToGeminiStreaming(prompt, onChunk, false); // useFreeTier = false
      return {
        text: fullResponse || 'Sin respuesta',
        provider: 'gemini',
        streaming: true
      };
    }

    case 'deepseek': {
      console.log('[callProviderStreaming] Iniciando streaming con DeepSeek');
      const fullResponse = await sendToDeepseekStreaming(prompt, onChunk, options.model || null);
      return {
        text: fullResponse || 'Sin respuesta',
        provider: 'deepseek',
        streaming: true
      };
    }

    case 'kimi': {
      console.log('[callProviderStreaming] Iniciando streaming con Kimi');
      const fullResponse = await sendToKimiStreaming(prompt, onChunk, options.model || null);
      return {
        text: fullResponse || 'Sin respuesta',
        provider: 'kimi',
        streaming: true
      };
    }

    case 'lmstudio': {
      console.log('[callProviderStreaming] Iniciando streaming con LM Studio');
      const fullResponse = await sendToLMStudioStreaming(prompt, onChunk);
      return {
        text: fullResponse || 'Sin respuesta',
        provider: 'lmstudio',
        streaming: true
      };
    }

    case 'ollama': {
      // Permitir override del modelo v칤a options.model > options.ragModel > settings
      const model = options.model || options.ragModel || settings.ollamaModel;
      if (!model) {
        throw new Error('No se ha seleccionado un modelo de Ollama en los ajustes.');
      }

      // Streaming: soportado si settings lo indica Y no es modelo RAG override
      // Con options.model (sesi칩n) mantenemos el supportsStreaming del estado local
      const useStreaming = settings.ollamaModelSupportsStreaming && !options.ragModel;

      if (useStreaming) {
        console.log(`[callProviderStreaming] Iniciando streaming con Ollama modelo: ${model}`);
        const fullResponse = await ollamaGenerateStreaming(model, prompt, onChunk);
        return {
          text: fullResponse || 'Sin respuesta',
          provider: 'ollama',
          model: model,
          streaming: true
        };
      }

      // Fallback: modo normal sin streaming (para modelos RAG o si no soporta streaming)
      console.log(`游댃 Usando modo no-streaming para Ollama${options.ragModel ? ` (RAG model: ${model})` : ''}`);
      const result = await callProvider(prompt, options);
      if (onChunk && result.text) {
        onChunk(result.text);
      }
      return {
        ...result,
        streaming: false
      };
    }

    default:
      // Fallback para cualquier otro proveedor
      console.log(`游댃 Usando modo no-streaming para ${provider}`);
      const result = await callProvider(prompt);
      if (onChunk && result.text) {
        onChunk(result.text);
      }
      return {
        ...result,
        streaming: false
      };
  }
}

// Re-exportar funciones 칰tiles
export { getDeepseekAvailableModels, getKimiAvailableModels, getLMStudioModels };
